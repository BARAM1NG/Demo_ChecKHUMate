{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNrL3shku5Lv",
        "outputId": "6df1fd5b-9a74-4cab-a1f4-f72d0b39a8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# 구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TD02YWqGAF6v"
      },
      "source": [
        "# 패키지 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xS9H0XcSvAoS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c28335f-e07f-4fcf-e4ff-078734c4e296"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-2.7.0-py3-none-any.whl (171 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 sentence-transformers-2.7.0\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.25.2)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.8.0\n"
          ]
        }
      ],
      "source": [
        "# 필요한 패키지 임포트\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import math\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import torch\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLVYcGDPzNt5"
      },
      "source": [
        "# 1-1. Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_uz7-YJZzVcZ"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/ChecKHUMate/merge_domitory_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJINIynT8A1A"
      },
      "source": [
        "# 1-2. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_data(data):\n",
        "    \"\"\"\n",
        "    주어진 데이터 프레임에서 필요한 정보를 추출하고 포맷팅하는 함수.\n",
        "\n",
        "    Parameters:\n",
        "        data (pd.DataFrame): 원본 데이터 프레임.\n",
        "\n",
        "    Returns:\n",
        "        user_data (pd.DataFrame): 사용자 데이터 프레임.\n",
        "        wish_data (pd.DataFrame): 사용자의 선호에 관한 데이터 프레임.\n",
        "    \"\"\"\n",
        "    # 사용자 데이터 선택\n",
        "    user_data_columns = [\n",
        "        'user_id', 'domitory', 'age', 'student_id', 'gender', 'major',\n",
        "        'bedtime', 'clean_duration', 'smoke', 'alcohol', 'mbti', 'one_sentence'\n",
        "    ]\n",
        "    user_data = data[user_data_columns]\n",
        "    user_data = user_data.set_index('user_id')\n",
        "\n",
        "    # 사용자 선호 데이터 선택\n",
        "    wish_data_columns = [\n",
        "        'user_id', 'wish_domitory', 'wish_age', 'wish_student_id', 'wish_gender',\n",
        "        'wish_major', 'wish_bedtime', 'wish_clean_duration', 'wish_smoke',\n",
        "        'wish_alcohol', 'wish_mbti'\n",
        "    ]\n",
        "    wish_data = data[wish_data_columns]\n",
        "    wish_data = wish_data.set_index('user_id')\n",
        "\n",
        "    return user_data, wish_data"
      ],
      "metadata": {
        "id": "UOeXDsm7d1pm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_data, wish_data = prepare_data(data)"
      ],
      "metadata": {
        "id": "z_Ye442Sd7lP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eA1N1FYZYsUv"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9l3PmZhfYVW3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "# 코랩용.\n",
        "def encode_and_concat_to_array(dataframe, column_name):\n",
        "    \"\"\"\n",
        "    원핫 인코딩을 수행하고 원본 데이터프레임에 결과를 추가한 후 전체를 numpy 배열로 반환합니다.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): 원본 데이터를 포함한 데이터프레임.\n",
        "    column_name (str): 원핫 인코딩을 적용할 열의 이름.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 원핫 인코딩된 결과와 원본 데이터가 결합된 numpy 배열.\n",
        "    \"\"\"\n",
        "    # 원핫 인코더 초기화 및 피팅\n",
        "    encoder = OneHotEncoder(sparse_output=False)  # 바로 numpy array 반환 설정\n",
        "    encoded_data = encoder.fit_transform(dataframe[[column_name]])\n",
        "\n",
        "    # 원핫 인코딩 결과를 DataFrame으로 변환\n",
        "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([column_name]))\n",
        "\n",
        "    # 데이터프레임의 인덱스 리셋\n",
        "    dataframe = dataframe.reset_index(drop=True)\n",
        "    encoded_df = encoded_df.reset_index(drop=True)\n",
        "\n",
        "    # 원본 데이터와 원핫 인코딩된 데이터를 결합\n",
        "    new_dataframe = pd.concat([dataframe, encoded_df], axis=1)\n",
        "\n",
        "    # 인코딩된 열 제거\n",
        "    new_dataframe = new_dataframe.drop(column_name, axis=1)\n",
        "\n",
        "    return new_dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "TAwC2RsT70RJ"
      },
      "outputs": [],
      "source": [
        "# DB 용\n",
        "def encode_and_concat_to_array(dataframe, column_name):\n",
        "    \"\"\"\n",
        "    원핫 인코딩을 수행하고 원본 데이터프레임에 결과를 추가한 후 전체를 numpy 배열로 반환합니다.\n",
        "\n",
        "    Args:\n",
        "    dataframe (pd.DataFrame): 원본 데이터를 포함한 데이터프레임.\n",
        "    column_name (str): 원핫 인코딩을 적용할 열의 이름.\n",
        "\n",
        "    Returns:\n",
        "    numpy.ndarray: 원핫 인코딩된 결과와 원본 데이터가 결합된 numpy 배열.\n",
        "    \"\"\"\n",
        "    # 원핫 인코더 초기화 및 피팅\n",
        "    encoder = OneHotEncoder(sparse_output=False)  # 바로 numpy array 반환 설정\n",
        "    encoded_data = encoder.fit_transform(dataframe[[column_name]])\n",
        "\n",
        "    # 원핫 인코딩 결과를 DataFrame으로 변환\n",
        "    encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out([column_name]))\n",
        "\n",
        "    # 원본 데이터와 원핫 인코딩된 데이터를 결합\n",
        "    new_dataframe = pd.concat([dataframe, encoded_df], axis=1)\n",
        "\n",
        "    new_dataframe = new_dataframe.drop(column_name, axis = 1)\n",
        "\n",
        "    # 전체 데이터프레임을 numpy 배열로 변환\n",
        "    final_array = new_dataframe.to_numpy()\n",
        "\n",
        "    return final_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a1f-l_NlhB6H"
      },
      "outputs": [],
      "source": [
        "user_df = encode_and_concat_to_array(user_data, 'mbti')\n",
        "wish_df = encode_and_concat_to_array(wish_data, 'wish_mbti')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "3EP02FamuSWw"
      },
      "outputs": [],
      "source": [
        "# Faiss 유사도 계산을 위한 데이터 전처리\n",
        "\n",
        "# 1) user_data에서 one_sentence 데이터 drop\n",
        "# 0행: 'user_id', 1행:'domitory', 2행: 'age', 3행: 'student_id', 4행: 'gender', 5행: 'major', 6행: 'bedtime', 7행: 'clean_duration', 8행:'smoke',\n",
        "# 9행: 'alcohol', 10행: 'one_sentence'\n",
        "\n",
        "# one_sentence drop\n",
        "user_df_a = user_df.drop(columns = 'one_sentence')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ky9mKggzcuB"
      },
      "source": [
        "# 2. Data Filtering & Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMbTLTk2ZUK"
      },
      "source": [
        "## 2-1 Faiss 유사도"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "udibJIk4s93_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# 각 사용자의 wish_data를 기반으로 user_data에서 자기 자신을 제외한 상위 k개의 가장 유사한 사용자들을 찾아낸 결과를 배열로 반환합니다.\n",
        "class Recommender_without_sentence:\n",
        "    def __init__(self, user_data, wish_data):\n",
        "        self.user_data = user_data\n",
        "        self.wish_data = wish_data\n",
        "        self.index = None\n",
        "        self.d = user_data.shape[1]\n",
        "\n",
        "    def build_index(self):\n",
        "        \"\"\"FAISS 인덱스를 생성하고 유저 데이터를 추가합니다.\"\"\"\n",
        "        self.index = faiss.IndexFlatL2(self.d)\n",
        "        self.index.add(self.user_data.astype('float32'))\n",
        "\n",
        "    def find_roommates(self, k=10):\n",
        "        \"\"\"각 유저의 wish_data에 대해 가장 유사한 유저를 찾되, 자기 자신은 제외하고, -1 인덱스가 발생한 경우도 기록합니다.\"\"\"\n",
        "        if self.index is None:\n",
        "            self.build_index()\n",
        "\n",
        "        all_results = []  # 결과를 저장할 배열\n",
        "        invalid_indices_info = []  # -1 인덱스가 발생한 정보를 저장할 배열\n",
        "\n",
        "        for i in range(self.wish_data.shape[0]):\n",
        "            # 자기 자신을 포함하여 k+1개의 결과를 검색\n",
        "            distances, indices = self.index.search(self.wish_data[i:i+1].astype('float32'), k+1)\n",
        "\n",
        "            # 자기 자신의 인덱스와 -1을 제외\n",
        "            valid_indices = indices[0][(indices[0] != i) & (indices[0] != -1)]\n",
        "\n",
        "            # -1이 발생한 경우 기록\n",
        "            if np.any(indices[0] == -1):\n",
        "                invalid_indices_info.append((i, list(indices[0])))\n",
        "\n",
        "            # k개의 결과만 반환\n",
        "            all_results.append(valid_indices[:k])\n",
        "\n",
        "        # 결과와 -1 정보 모두 반환\n",
        "        return np.array(all_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "2wuEp5Soy_-E"
      },
      "outputs": [],
      "source": [
        "recommender = Recommender_without_sentence(user_df_a, wish_df)\n",
        "roommate_recommendations = recommender.find_roommates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jXYWLGmBzfUv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 구버전\n",
        "class IntroductionRecommender:\n",
        "    def __init__(self, user_data):\n",
        "        self.user_data = user_data\n",
        "        self.embedder = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\n",
        "        self.index = None\n",
        "        self.embeddings = None\n",
        "        self.user_data['one_sentence'] = user_data['one_sentence'].fillna('No introduction provided').astype(str)\n",
        "\n",
        "    def create_faiss_index(self):\n",
        "        \"\"\"임베딩을 생성하고 FAISS 인덱스에 추가합니다.\"\"\"\n",
        "        # 모든 자기소개 문장 임베딩 생성\n",
        "        self.embeddings = self.embedder.encode(self.user_data['one_sentence'].tolist(), convert_to_tensor=False)\n",
        "\n",
        "        # FAISS 인덱스 생성\n",
        "        d = self.embeddings.shape[1]  # 임베딩 차원\n",
        "        self.index = faiss.IndexFlatL2(d)\n",
        "        self.index.add(self.embeddings.astype('float32'))\n",
        "\n",
        "    def find_similar_introductions(self, result_indices, k=10):\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"FAISS index is not built. Please run create_faiss_index method first.\")\n",
        "\n",
        "        all_group_similarities = []\n",
        "\n",
        "        for indices in result_indices:\n",
        "            min_distances = {}  # 인덱스별 최소 거리를 저장할 사전\n",
        "\n",
        "            for element in indices:\n",
        "                query_embedding = self.embeddings[element]\n",
        "                D, I = self.index.search(query_embedding.reshape(1, -1), k+1)\n",
        "\n",
        "                for i, d in zip(I[0], D[0]):\n",
        "                    if i != element and i in indices:\n",
        "                        if i not in min_distances or min_distances[i] > d:\n",
        "                            min_distances[i] = d  # 최소 거리 업데이트\n",
        "\n",
        "            # 인덱스와 거리를 튜플로 변환하여 리스트로 만든 후, 거리에 따라 정렬\n",
        "            group_similarities = sorted(min_distances.items(), key=lambda x: x[1])\n",
        "            all_group_similarities.append(group_similarities)\n",
        "\n",
        "        return all_group_similarities"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# 신버전\n",
        "class IntroductionRecommender:\n",
        "    def __init__(self, user_data):\n",
        "        self.user_data = user_data\n",
        "        self.embedder = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
        "        self.index = None\n",
        "        self.id_index = np.array(self.user_data.index)\n",
        "        self.embeddings = None\n",
        "        self.user_data['one_sentence'] = user_data['one_sentence'].fillna('No introduction provided').astype(str)\n",
        "\n",
        "    def create_faiss_index(self):\n",
        "\n",
        "        \"\"\"임베딩을 생성하고 FAISS 인덱스에 추가합니다.\"\"\"\n",
        "        # 모든 자기소개 문장 임베딩 생성\n",
        "        self.embeddings = self.embedder.encode(self.user_data['one_sentence'].tolist(), convert_to_tensor=False)\n",
        "\n",
        "        normalized_embeddings = self.embeddings.copy()\n",
        "\n",
        "        # normalized_embeddings의 벡터를 L2 Norm(유클리드 거리 기준)으로 정규화합니다.\n",
        "        # 이렇게 함으로써, 벡터의 길이를 1로 만들어 줍니다. 이것은 내적 기반의 유사도 검색에서 중요한 절차입니다.\n",
        "        # 왜냐하면 정규화된 벡터들 사이의 내적은 두 벡터의 코사인 유사도와 동일하기 때문입니다.\n",
        "        faiss.normalize_L2(normalized_embeddings)\n",
        "\n",
        "        # FAISS 인덱스 생성\n",
        "        d = self.embeddings.shape[1]  # 임베딩 차원\n",
        "        index_flat = faiss.IndexFlatIP(d)\n",
        "\n",
        "        self.index =index = faiss.IndexIDMap(index_flat)\n",
        "        self.index.add_with_ids(normalized_embeddings, self.id_index)\n",
        "\n",
        "\n",
        "    def find_similar_introductions(self, result_indices, k=10):\n",
        "        if self.index is None:\n",
        "            raise ValueError(\"FAISS index is not built. Please run create_faiss_index method first.\")\n",
        "\n",
        "        all_group_similarities = []\n",
        "\n",
        "        for indices in result_indices:\n",
        "            min_distances = {}  # 인덱스별 최소 거리를 저장할 사전\n",
        "\n",
        "            for element in indices:\n",
        "                # 인덱스에 해당하는 텍스트만 인코딩합니다.\n",
        "                text_to_encode = [self.user_data['one_sentence'].iloc[element]]\n",
        "                vector = self.embedder.encode(text_to_encode)\n",
        "                faiss.normalize_L2(vector)\n",
        "\n",
        "                D, I = self.index.search(vector.reshape(1, -1), k+1)\n",
        "\n",
        "                for i, d in zip(I[0], D[0]):\n",
        "                    if i != element and i in indices:\n",
        "                        if i not in min_distances or min_distances[i] > d:\n",
        "                            min_distances[i] = d  # 최소 거리 업데이트\n",
        "\n",
        "\n",
        "            # 인덱스와 거리를 튜플로 변환하여 리스트로 만든 후, 텍스트 유사도에 따라\n",
        "            group_similarities = sorted(min_distances.items(), key=lambda x: x[1], reverse = True)\n",
        "            all_group_similarities.append(group_similarities)\n",
        "\n",
        "        return all_group_similarities"
      ],
      "metadata": {
        "id": "nGDUn9CH4hU9"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommender = IntroductionRecommender(user_df)\n",
        "recommender.create_faiss_index()\n",
        "similarities = recommender.find_similar_introductions(roommate_recommendations)\n",
        "print(similarities[102])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5kH4K0vMFLo",
        "outputId": "88a444ad-3e03-4046-db47-e69271959a3c"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(100, 0.6553358), (106, 0.6553357), (103, 0.48954824), (97, 0.48547697), (88, 0.48547685), (107, 0.4410506), (105, 0.41970214), (94, 0.3905151)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "roommate_recommendations[102]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7xo1PkhURNM",
        "outputId": "11edf717-1b14-4dd9-e02a-1065dde4d196"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 88, 103, 113,  94, 100, 105, 106,  96,  97, 107])"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_data.iloc[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bPUb8G2XpFD",
        "outputId": "c5e29386-8a00-4fca-9133-176d5020c15b"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "domitory                                      2\n",
              "age                                           4\n",
              "student_id                                    4\n",
              "gender                                        0\n",
              "major                                         1\n",
              "bedtime                                       3\n",
              "clean_duration                                1\n",
              "smoke                                         0\n",
              "alcohol                                       1\n",
              "mbti                                       ISFP\n",
              "one_sentence      방에 타인이 들어오는 걸 별로 안 좋아하는 편입니다.\n",
              "Name: 101, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wish_data.iloc[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiYD_MM4ZIQE",
        "outputId": "fd00a14c-ec52-4a6e-80fa-5650a285cc8b"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "wish_domitory             2\n",
              "wish_age                  4\n",
              "wish_student_id           4\n",
              "wish_gender               0\n",
              "wish_major                1\n",
              "wish_bedtime              3\n",
              "wish_clean_duration       1\n",
              "wish_smoke                0\n",
              "wish_alcohol              1\n",
              "wish_mbti              ENTP\n",
              "Name: 101, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_data.iloc[102]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-aQIF4DXzSJ",
        "outputId": "a39914b5-dc57-4e63-8e75-c531edc4937a"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "domitory                       2\n",
              "age                            4\n",
              "student_id                     4\n",
              "gender                         0\n",
              "major                          2\n",
              "bedtime                        3\n",
              "clean_duration                 0\n",
              "smoke                          0\n",
              "alcohol                        0\n",
              "mbti                        ISTP\n",
              "one_sentence      독립성이 강한 성격입니다.\n",
              "Name: 103, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}